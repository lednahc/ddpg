{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Continuous Control - Report\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "### DDPG\n",
    "\n",
    "-> DDPG algorithm due to continous actions.\n",
    "\n",
    "-> For implementation, I took help of the sample code provided by Udacity for this assignment and OpenAI baselines.\n",
    "\n",
    "-> Actor Network - 2 layers, 128 and 64.\n",
    "\n",
    "-> Critic Network - 2 layers, 128 and 64.\n",
    "\n",
    "-> Used Adam optim.\n",
    "\n",
    "-> Batch size: 1024, gamma: 0.99, tau: 1e-3\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Following hyperparameters have been used to train the network:\n",
    "\n",
    "\n",
    "- BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "- BATCH_SIZE = 1024       # minibatch size\n",
    "- GAMMA = 0.99            # discount factor\n",
    "- TAU = 1e-3              # for soft update of target parameters\n",
    "- LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "- LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "- WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "\n",
    "## Plot of Rewards\n",
    "```\n",
    "Episode number: 0 | Last hundred average: 1.94 | Current episode average: 1.94\n",
    "Episode number: 10 | Last hundred average: 3.06 | Current episode average: 5.07\n",
    "Episode number: 20 | Last hundred average: 2.86 | Current episode average: 7.21\n",
    "Episode number: 30 | Last hundred average: 3.70 | Current episode average: 3.64\n",
    "Episode number: 40 | Last hundred average: 5.35 | Current episode average: 10.16\n",
    "Episode number: 50 | Last hundred average: 6.51 | Current episode average: 12.55\n",
    "Episode number: 60 | Last hundred average: 7.86 | Current episode average: 15.09\n",
    "Episode number: 70 | Last hundred average: 9.40 | Current episode average: 21.94\n",
    "Episode number: 80 | Last hundred average: 10.44 | Current episode average: 19.15\n",
    "Episode number: 90 | Last hundred average: 11.72 | Current episode average: 27.27\n",
    "Episode number: 100 | Last hundred average: 13.36 | Current episode average: 33.55\n",
    "Episode number: 110 | Last hundred average: 15.73 | Current episode average: 26.45\n",
    "Episode number: 120 | Last hundred average: 18.01 | Current episode average: 19.50\n",
    "Episode number: 130 | Last hundred average: 20.08 | Current episode average: 24.56\n",
    "Episode number: 140 | Last hundred average: 22.33 | Current episode average: 29.12\n",
    "Episode number: 150 | Last hundred average: 24.04 | Current episode average: 33.92\n",
    "Episode number: 160 | Last hundred average: 25.82 | Current episode average: 32.77\n",
    "Episode number: 170 | Last hundred average: 26.89 | Current episode average: 26.63\n",
    "Episode number: 180 | Last hundred average: 28.00 | Current episode average: 29.77\n",
    "Episode number: 190 | Last hundred average: 28.83 | Current episode average: 24.04\n",
    "Episode number: 200 | Last hundred average: 28.94 | Current episode average: 22.53\n",
    "Episode number: 210 | Last hundred average: 28.88 | Current episode average: 25.76\n",
    "Episode number: 220 | Last hundred average: 29.33 | Current episode average: 31.16\n",
    "Episode number: 230 | Last hundred average: 29.47 | Current episode average: 23.03\n",
    "Episode number: 240 | Last hundred average: 28.91 | Current episode average: 29.41\n",
    "Episode number: 250 | Last hundred average: 29.17 | Current episode average: 34.71\n",
    "Episode number: 260 | Last hundred average: 29.32 | Current episode average: 36.73\n",
    "Episode number: 270 | Last hundred average: 29.92 | Current episode average: 36.73\n",
    "Episode number: 280 | Last hundred average: 30.52 | Current episode average: 34.14\n",
    "Episode number: 290 | Last hundred average: 31.00 | Current episode average: 37.46\n",
    "```\n",
    "\n",
    "\n",
    "![alt text](rewards.png \"Left\")\n",
    "\n",
    "\n",
    "## Ideas for future work\n",
    "\n",
    "Experimentation using\n",
    "\n",
    "* PPO\n",
    "* A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
